{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset and preprocess it as needed\n",
    "# X, y = load_data()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define a baseline neural network model\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the baseline model\n",
    "baseline_model = baseline_model()\n",
    "baseline_model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1, validation_split=0.2)\n",
    "baseline_scores = baseline_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Baseline Accuracy: {baseline_scores[1]*100:.2f}%\")\n",
    "\n",
    "# Define a function to create a neural network model with hyperparameters\n",
    "def create_model(learning_rate=0.001, dropout_rate=0.2, weight_decay=1e-4):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=keras.regularizers.l1_l2(weight_decay)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l1_l2(weight_decay)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Hyperparameter tuning using RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'dropout_rate': [0.2, 0.3, 0.4],\n",
    "    'weight_decay': [1e-4, 1e-5, 1e-6],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [20, 30, 40]\n",
    "}\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=kfold, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "random_search_results = random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Accuracy: {random_search_results.best_score_:.2f}%\")\n",
    "print(f\"Best Parameters: {random_search_results.best_params_}\")\n",
    "\n",
    "# Neural network with optimal hyperparameters\n",
    "best_model = random_search_results.best_estimator_\n",
    "best_model.fit(X_train, y_train, epochs=best_model.epochs, batch_size=best_model.batch_size, verbose=1, validation_split=0.2)\n",
    "best_scores = best_model.model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Best Model Accuracy: {best_scores[1]*100:.2f}%\")\n",
    "\n",
    "# Cross-validation with the best model\n",
    "cv_scores = []\n",
    "for train, val in kfold.split(X_train, y_train):\n",
    "    cv_model = create_model(learning_rate=best_model.learning_rate, dropout_rate=best_model.dropout_rate, weight_decay=best_model.weight_decay)\n",
    "    cv_model.fit(X_train[train], y_train[train], epochs=best_model.epochs, batch_size=best_model.batch_size, verbose=0)\n",
    "    _, accuracy = cv_model.evaluate(X_train[val], y_train[val], verbose=0)\n",
    "    cv_scores.append(accuracy)\n",
    "\n",
    "print(f\"Cross-Validation Accuracy: {np.mean(cv_scores)*100:.2f}%\")\n",
    "\n",
    "# Ensemble methods using Bagging\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagging_model = BaggingClassifier(base_estimator=best_model, n_estimators=10, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "bagging_accuracy = bagging_model.score(X_test, y_test)\n",
    "print(f\"Bagging Accuracy: {bagging_accuracy*100:.2f}%\")\n",
    "\n",
    "# Learning rate scheduling\n",
    "def learning_rate_schedule(epoch, learning_rate):\n",
    "    if epoch < 10:\n",
    "        return learning_rate\n",
    "    else:\n",
    "        return learning_rate * tf.math.exp(-0.1)\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(learning_rate_schedule)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "model = create_model()\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[lr_schedule, early_stopping, model_checkpoint])\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
